{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#################### Includes modules\n",
    "import numpy as np\n",
    "import glob as gl\n",
    "import csv\n",
    "import pandas\n",
    "import h5py\n",
    "import os\n",
    "from decimal import Decimal, getcontext\n",
    "import tensorflow as tf\n",
    "\n",
    "getcontext().prec = 1\n",
    "FILES_WITH_DATA = gl.glob(\"../Data/*.mat\")\n",
    "\n",
    "#################### Date Set Class\n",
    "# Classification types: 1 -> for loading files with data from the scope of valve opening range = 5%\n",
    "#                       2 -> for files with samples for every 1% of valve opening range\n",
    "#                       3 -> for loading all files\n",
    "#                       4 -> for loading data from files with valve opening range every 10%\n",
    "#                       5 -> for date tables for conv nn with generall classification of 1 type\n",
    "# Each sensor have id (N), if sensorN will get value 1, then it takes part in dataset collection.\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, classification_type = 1, data_number_in_sample = 2000, train_to_valid_size = 0.8, conv_dataset = False, conv_axis_X = 43,  sensor1 = 1, sensor2 = 1,  sensor3 = 1, sensor4 = 1,  sensor5 = 1, sensor6 = 1,  sensor7 = 1):\n",
    "        self.classification_type = classification_type\n",
    "        self.data_number_in_sample = data_number_in_sample\n",
    "        self.train_size = int(data_number_in_sample * train_to_valid_size) // 1\n",
    "        self.valid_size = int(data_number_in_sample * (Decimal(1) - Decimal(train_to_valid_size))) // 1\n",
    "        self.conv_axis_X = conv_axis_X\n",
    "        self.conv_dataset = conv_dataset\n",
    "        self.sensor1 = sensor1\n",
    "        self.sensor2 = sensor2\n",
    "        self.sensor3 = sensor3\n",
    "        self.sensor4 = sensor4\n",
    "        self.sensor5 = sensor5\n",
    "        self.sensor6 = sensor6\n",
    "        self.sensor7 = sensor7\n",
    "        self.number_of_sensor_in_use = sensor1 + sensor2 + sensor3 + sensor4 + sensor5 + sensor6 + sensor7\n",
    "        if conv_dataset:\n",
    "            self.training_data = np.zeros([(self.train_size * self.samples_number(classification_type)), self.conv_axis_X, self.number_of_sensor_in_use])\n",
    "            self.validation_data = np.zeros([(self.valid_size * self.samples_number(classification_type)), self.conv_axis_X, self.number_of_sensor_in_use])\n",
    "        else:\n",
    "            self.training_data = np.zeros([(self.train_size * self.samples_number(classification_type)), self.number_of_sensor_in_use])\n",
    "            self.validation_data = np.zeros([(self.valid_size * self.samples_number(classification_type)), self.number_of_sensor_in_use])\n",
    "        self.training_valve_opening_scopes = np.zeros((self.train_size * self.samples_number(classification_type)))\n",
    "        self.validation_valve_opening_scopes = np.zeros((self.valid_size * self.samples_number(classification_type)))\n",
    "        self.sort_files_with_data()\n",
    "        self.data_reader()\n",
    "\n",
    "\n",
    "    def samples_number(parent, classification_type):\n",
    "        if parent.classification_type == 1:\n",
    "            return int(20)\n",
    "\n",
    "    def data_reader(parent):\n",
    "        if parent.classification_type == 1:\n",
    "            parent.load_data_for_classification_1()\n",
    "            parent.one_hot_encoding(1)\n",
    "        elif parent.classification_type == 5:\n",
    "            parent.load_conv_data_for_classification_1()\n",
    "            parent.one_hot_encoding(1)\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "\n",
    "    def create_sample_training_set(parent, data, start_index, train_size, valve_opening, conv_axis_x):\n",
    "        dataset_iter = 0\n",
    "        for i in range(start_index, start_index + train_size):\n",
    "            for x in range(conv_axis_x):\n",
    "                for j in range(parent.number_of_sensor_in_use):\n",
    "                    if parent.conv_dataset == True:\n",
    "                        parent.training_data[i][x][j] = data[dataset_iter][j]\n",
    "                    else:\n",
    "                        parent.training_data[i][j] = data[dataset_iter][j]\n",
    "                parent.training_valve_opening_scopes[i] = int(valve_opening // 1)\n",
    "                dataset_iter += 1\n",
    "\n",
    "    def create_sample_valid_set(parent, data, start_index, train_size, valid_size, valve_opening, conv_axis_x):\n",
    "        dataset_iter = train_size\n",
    "        for i in range(start_index, start_index + valid_size):\n",
    "            for x in range(conv_axis_x):\n",
    "                for j in range(parent.number_of_sensor_in_use):\n",
    "                    if parent.conv_dataset == True:\n",
    "                        parent.validation_data[i][x][j] = data[dataset_iter][j]\n",
    "                    else:\n",
    "                        parent.validation_data[i][j] = data[dataset_iter][j]\n",
    "                parent.validation_valve_opening_scopes[i] = int(valve_opening // 1)\n",
    "                dataset_iter += 1\n",
    "\n",
    "    def load_data_for_classification_1(parent):\n",
    "        last_valve_value = 0\n",
    "        train_start_indx = 0\n",
    "        valid_start_indx = 0\n",
    "        for f in FILES_WITH_DATA:\n",
    "            valve_value = float(os.path.basename(f)[:-4])\n",
    "            if (valve_value // 1 % 5 == 0):\n",
    "                if ((last_valve_value // 1) != (valve_value // 1)):\n",
    "                    tmp_array_of_full_data = np.array(h5py.File(f).get('p').value)\n",
    "                    parent.create_sample_training_set(tmp_array_of_full_data, train_start_indx, parent.train_size, valve_value, 1)\n",
    "                    parent.create_sample_valid_set(tmp_array_of_full_data, valid_start_indx, parent.train_size, parent.valid_size, valve_value, 1)\n",
    "                    train_start_indx += parent.train_size\n",
    "                    #print(train_start_indx)\n",
    "                    valid_start_indx += parent.valid_size\n",
    "                    #print(valid_start_indx)\n",
    "                    last_valve_value = valve_value\n",
    "                    \n",
    "    def load_conv_data_for_classification_1(parent):\n",
    "        last_valve_value = 0\n",
    "        train_start_indx = 0\n",
    "        valid_start_indx = 0\n",
    "        for f in FILES_WITH_DATA:\n",
    "            valve_value = float(os.path.basename(f)[:-4])\n",
    "            if (valve_value // 1 % 5 == 0):\n",
    "                if ((last_valve_value // 1) != (valve_value // 1)):\n",
    "                    tmp_array_of_full_data = np.array(h5py.File(f).get('p').value)\n",
    "                    parent.create_sample_training_set(tmp_array_of_full_data, train_start_indx, parent.train_size, valve_value, self.conv_axis_X)\n",
    "                    parent.create_sample_valid_set(tmp_array_of_full_data, valid_start_indx, parent.train_size, parent.valid_size, valve_value, self.conv_axis_X)\n",
    "                    train_start_indx += parent.train_size\n",
    "                    valid_start_indx += parent.valid_size\n",
    "                    last_valve_value = valve_value\n",
    "\n",
    "    def sort_files_with_data(parent):\n",
    "        for j in range(len(FILES_WITH_DATA)):\n",
    "            for i in range(len(FILES_WITH_DATA)-1):\n",
    "                if (float(os.path.basename(FILES_WITH_DATA[i])[:-4]) > float(os.path.basename(FILES_WITH_DATA[i + 1])[:-4])):\n",
    "                    tmp = FILES_WITH_DATA[i + 1]\n",
    "                    FILES_WITH_DATA[i + 1] = FILES_WITH_DATA[i]\n",
    "                    FILES_WITH_DATA[i] = tmp\n",
    "\n",
    "    def one_hot_encoding(parent, classification_type):\n",
    "        if (classification_type == 1):\n",
    "            for i in range(len(parent.training_valve_opening_scopes)):\n",
    "                parent.training_valve_opening_scopes[i] = parent.training_valve_opening_scopes[i] / 5\n",
    "            for i in range(len(parent.validation_valve_opening_scopes)):\n",
    "                parent.validation_valve_opening_scopes[i] = parent.validation_valve_opening_scopes[i] / 5\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "        tf.keras.utils.to_categorical(parent.training_valve_opening_scopes) \n",
    "        tf.keras.utils.to_categorical(parent.validation_valve_opening_scopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = Dataset() \n",
    "#dt = Dataset(data_number_in_sample = 20, conv_dataset = True)\n",
    "# print(FILES_WITH_DATA.type())\n",
    "# print(FILES_WITH_DATA)\n",
    "# print(\"===========\")\n",
    "#print(dt.training_data)\n",
    "# print(\"===========\")\n",
    "# print(dt.validation_valve_opening_scopes)\n",
    "# print(\"===========\")\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "#print(dt.validation_data)\n",
    "#print(dt.validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.00104455 0.53885013 5.09477751 1.7997669  1.33391057 0.22032908\n",
      "  0.44148675]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dt.validation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_valve_value = 0\n",
    "    train_start_indx = 0\n",
    "    valid_start_indx = 0\n",
    "    for f in FILES_WITH_DATA:\n",
    "        valve_value = float(os.path.basename(f)[:-4])\n",
    "        if (valve_value // 1 % 5 == 0):\n",
    "            if ((last_valve_value // 1) != (valve_value // 1)):\n",
    "                tmp_array_of_full_data = np.array(h5py.File(f).get('p').value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import glob as gl\n",
    "# import csv\n",
    "# import pandas\n",
    "# import h5py\n",
    "# from decimal import Decimal, getcontext\n",
    "# files = gl.glob(\"../Data/*.txt\")\n",
    "# last = \"c\"\n",
    "# for f in files:\n",
    "#     os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import glob as gl\n",
    "# import csv\n",
    "# import pandas\n",
    "# import h5py\n",
    "# from decimal import Decimal, getcontext\n",
    "# files = gl.glob(\"../Data/*.mat\")\n",
    "# oldname = \"\"\n",
    "# for f in files:\n",
    "#     l = []\n",
    "#     newname = f[f.find(\"TOA\") + 3 : f.find(\"_a_\")]\n",
    "#     if oldname == newname:\n",
    "#         oldname = newname\n",
    "#     else:\n",
    "#         #l.append(newname)\n",
    "#         #l.append(r\"C:\\Users\\Kowalio\\Desktop\\Badania SI maszyn przep≈Çywowych\\Data\")\n",
    "#         #s = ''.join(1)\n",
    "#         os.rename(f, newname + '.mat')\n",
    "#         oldname = newname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
